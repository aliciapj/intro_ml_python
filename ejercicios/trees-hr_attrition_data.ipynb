{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR attrition data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be using IBM Watson's HR Attrition data (the data has been utilized in the book after taking prior permission from the data administrator) shared in Kaggle datasets under open source license agreement https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset to predict whether employees would attrite or not based on independent explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "hrattr_data = pd.read_csv(\"../datasets/WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n",
    " \n",
    "hrattr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 1470 observations and 35 variables in this data, the top five rows are shown here for a quick glance of the variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to convert Yes or No categories into 1 and 0 for modeling purposes, as scikit-learn does not fit the model on character/categorical variables directly, hence dummy coding is required to be performed for utilizing the variables in models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrattr_data['Attrition_ind'] = 0 \n",
    "hrattr_data.loc[hrattr_data['Attrition'] =='Yes', 'Attrition_ind'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables are created for all seven categorical variables (shown here in alphabetical order), which are `Business Travel`, `Department`, `Education Field`, `Gender`, `Job Role`, `Marital Status`, and `Overtime`. We have ignored four variables from the analysis, as they do not change across the observations, which are Employee count, Employee number, Over18, and Standard Hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_busnstrvl = pd.get_dummies(hrattr_data['BusinessTravel'], prefix='busns_trvl') \n",
    "dummy_dept = pd.get_dummies(hrattr_data['Department'], prefix='dept') \n",
    "dummy_edufield = pd.get_dummies(hrattr_data['EducationField'], prefix='edufield') \n",
    "dummy_gender = pd.get_dummies(hrattr_data['Gender'], prefix='gend') \n",
    "dummy_jobrole = pd.get_dummies(hrattr_data['JobRole'], prefix='jobrole') \n",
    "dummy_maritstat = pd.get_dummies(hrattr_data['MaritalStatus'], prefix='maritalstat')  \n",
    "dummy_overtime = pd.get_dummies(hrattr_data['OverTime'], prefix='overtime') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous variables are separated and will be combined with the created dummy variables later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = ['Age','DailyRate','DistanceFromHome', 'Education', 'EnvironmentSatisfaction',\n",
    "                      'HourlyRate','JobInvolvement','JobLevel','JobSatisfaction', 'MonthlyIncome', \n",
    "                      'MonthlyRate', 'NumCompaniesWorked','PercentSalaryHike',  'PerformanceRating', \n",
    "                      'RelationshipSatisfaction','StockOptionLevel', 'TotalWorkingYears', \n",
    "                      'TrainingTimesLastYear','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', \n",
    "                      'YearsSinceLastPromotion','YearsWithCurrManager'] \n",
    " \n",
    "hrattr_continuous = hrattr_data[continuous_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, both derived dummy variables from categorical variables and straight continuous variables are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrattr_data_new = pd.concat([dummy_busnstrvl, dummy_dept, dummy_edufield, dummy_gender, dummy_jobrole, \n",
    "                             dummy_maritstat, dummy_overtime, hrattr_continuous, hrattr_data['Attrition_ind']],\n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>busns_trvl_Non-Travel</th>\n",
       "      <th>busns_trvl_Travel_Frequently</th>\n",
       "      <th>busns_trvl_Travel_Rarely</th>\n",
       "      <th>dept_Human Resources</th>\n",
       "      <th>dept_Research &amp; Development</th>\n",
       "      <th>dept_Sales</th>\n",
       "      <th>edufield_Human Resources</th>\n",
       "      <th>edufield_Life Sciences</th>\n",
       "      <th>edufield_Marketing</th>\n",
       "      <th>edufield_Medical</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>Attrition_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   busns_trvl_Non-Travel  busns_trvl_Travel_Frequently  \\\n",
       "0                      0                             0   \n",
       "1                      0                             1   \n",
       "2                      0                             0   \n",
       "3                      0                             1   \n",
       "4                      0                             0   \n",
       "\n",
       "   busns_trvl_Travel_Rarely  dept_Human Resources  \\\n",
       "0                         1                     0   \n",
       "1                         0                     0   \n",
       "2                         1                     0   \n",
       "3                         0                     0   \n",
       "4                         1                     0   \n",
       "\n",
       "   dept_Research & Development  dept_Sales  edufield_Human Resources  \\\n",
       "0                            0           1                         0   \n",
       "1                            1           0                         0   \n",
       "2                            1           0                         0   \n",
       "3                            1           0                         0   \n",
       "4                            1           0                         0   \n",
       "\n",
       "   edufield_Life Sciences  edufield_Marketing  edufield_Medical  ...  \\\n",
       "0                       1                   0                 0  ...   \n",
       "1                       1                   0                 0  ...   \n",
       "2                       0                   0                 0  ...   \n",
       "3                       1                   0                 0  ...   \n",
       "4                       0                   0                 1  ...   \n",
       "\n",
       "   RelationshipSatisfaction  StockOptionLevel  TotalWorkingYears  \\\n",
       "0                         1                 0                  8   \n",
       "1                         4                 1                 10   \n",
       "2                         2                 0                  7   \n",
       "3                         3                 0                  8   \n",
       "4                         4                 1                  6   \n",
       "\n",
       "   TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                      0                1               6                   4   \n",
       "1                      3                3              10                   7   \n",
       "2                      3                3               0                   0   \n",
       "3                      3                3               8                   7   \n",
       "4                      3                3               2                   2   \n",
       "\n",
       "   YearsSinceLastPromotion  YearsWithCurrManager  Attrition_ind  \n",
       "0                        0                     5              1  \n",
       "1                        1                     7              0  \n",
       "2                        0                     0              1  \n",
       "3                        3                     0              0  \n",
       "4                        2                     2              0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrattr_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Here, we have not removed one extra derived dummy variable for each categorical variable due to the reason that multi-collinearity does not create a problem in decision trees as it does in either logistic or linear regression, hence we can simply utilize all the derived variables in the rest of the chapter, as all the models utilize decision trees as an underlying model, even after performing ensembles of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once basic data has been prepared, it needs to be split by 70-30 for training and testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test split \n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train,x_test,y_train,y_test = \\\n",
    "    train_test_split(hrattr_data_new.drop (['Attrition_ind'], axis=1),\n",
    "                     hrattr_data_new['Attrition_ind'],   \n",
    "                     test_size = 0.3, \n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTtreeClassifier from scikit-learn has been utilized for modeling purposes, which is available in the tree submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters selected for the DT classifier are in the following code with splitting criterion as Gini, Maximum depth as 5, minimum number of observations required for qualifying split is 2, and the minimum samples that should be present in the terminal node is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree - Train Confusion  Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          844   9\n",
      "1           98  78\n",
      "\n",
      "Decision Tree - Train accuracy\n",
      "\n",
      " 0.896\n",
      "\n",
      "Decision Tree - Train Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       853\n",
      "           1       0.90      0.44      0.59       176\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1029\n",
      "   macro avg       0.90      0.72      0.77      1029\n",
      "weighted avg       0.90      0.90      0.88      1029\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree - Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          361  19\n",
      "1           49  12\n",
      "\n",
      "Decision Tree - Test accuracy 0.846\n",
      "\n",
      "Decision Tree - Test Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       380\n",
      "           1       0.39      0.20      0.26        61\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       441\n",
      "   macro avg       0.63      0.57      0.59       441\n",
      "weighted avg       0.81      0.85      0.82       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_fit = DecisionTreeClassifier(criterion=\"gini\", max_depth=5,min_samples_split=2,  min_samples_leaf=1,random_state=42) \n",
    "dt_fit.fit(x_train,y_train) \n",
    " \n",
    "print (\"\\nDecision Tree - Train Confusion  Matrix\\n\\n\", pd.crosstab(y_train, dt_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))    \n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report    \n",
    "print (\"\\nDecision Tree - Train accuracy\\n\\n\",round(accuracy_score (y_train, dt_fit.predict(x_train)),3)) \n",
    "print (\"\\nDecision Tree - Train Classification Report\\n\", classification_report(y_train, dt_fit.predict(x_train))) \n",
    " \n",
    "print (\"\\n\\nDecision Tree - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, dt_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"])) \n",
    "print (\"\\nDecision Tree - Test accuracy\",round(accuracy_score(y_test, dt_fit.predict(x_test)),3)) \n",
    "print (\"\\nDecision Tree - Test Classification Report\\n\", classification_report( y_test, dt_fit.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By carefully observing the results, we can infer that, even though the test accuracy is high (84.6%), the precision and recall of one category (Attrition = Yes) is low (precision = 0.39 and recall = 0.20). This could be a serious issue when management tries to use this model to provide some extra benefits proactively to the employees with a high chance of attrition prior to actual attrition, as this model is unable to identify the real employees who will be leaving. Hence, we need to look for other modifications; one way is to control the model by using class weights. By utilizing class weights, we can increase the importance of a particular class at the cost of an increase in other errors.\n",
    "\n",
    "For example, by increasing class weight to category 1, we can identify more employees with the characteristics of actual attrition, but by doing so, we will mark some of the non-potential churner employees as potential attriters (which should be acceptable).\n",
    "\n",
    "Another classical example of the important use of class weights is, in banking scenarios. When giving loans, it is better to reject some good applications than accepting bad loans. Hence, even in this case, it is a better idea to use higher weightage to defaulters over non-defaulters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning class weights in decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, class weights are tuned to see the performance change in decision trees with the same parameters. A dummy DataFrame is created to save all the results of various precision-recall details of combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dummyarray = np.empty((6,10))\n",
    "dt_wttune = pd.DataFrame(dummyarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics to be considered for capture are weight for zero and one category (for example, if the weight for zero category given is 0.2, then automatically, weight for the one should be 0.8, as total weight should be equal to 1), training and testing accuracy, precision for zero category, one category, and overall. Similarly, recall for zero category, one category, and overall are also calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_wttune.columns = [\"zero_wght\",\"one_wght\",\"tr_accuracy\", \"tst_accuracy\", \"prec_zero\",\"prec_one\", \"prec_ovll\", \n",
    "                     \"recl_zero\",\"recl_one\",\"recl_ovll\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights for the zero category are verified from 0.01 to 0.5, as we know we do not want to explore cases where the zero category will be given higher weightage than one category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Weights {0: 0.01, 1: 0.99} Train accuracy: 0.342 Test accuracy: 0.272\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted   0    1\n",
      "Actuall           \n",
      "0          65  315\n",
      "1           6   55\n",
      "\n",
      "Class Weights {0: 0.1, 1: 0.9} Train accuracy: 0.806 Test accuracy: 0.732\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          282  98\n",
      "1           20  41\n",
      "\n",
      "Class Weights {0: 0.2, 1: 0.8} Train accuracy: 0.871 Test accuracy: 0.83\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          341  39\n",
      "1           36  25\n",
      "\n",
      "Class Weights {0: 0.3, 1: 0.7} Train accuracy: 0.881 Test accuracy: 0.839\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          345  35\n",
      "1           36  25\n",
      "\n",
      "Class Weights {0: 0.4, 1: 0.6} Train accuracy: 0.894 Test accuracy: 0.832\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          346  34\n",
      "1           40  21\n",
      "\n",
      "Class Weights {0: 0.5, 1: 0.5} Train accuracy: 0.896 Test accuracy: 0.846\n",
      "Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          361  19\n",
      "1           49  12\n"
     ]
    }
   ],
   "source": [
    "zero_clwghts = [0.01,0.1,0.2,0.3,0.4,0.5] \n",
    "\n",
    "for i in range(len(zero_clwghts)):\n",
    "    \n",
    "    clwght = {0:zero_clwghts[i],1:1.0-zero_clwghts[i]}\n",
    "    dt_fit = DecisionTreeClassifier(criterion=\"gini\",max_depth=5,min_samples_split=2,\n",
    "                                    min_samples_leaf=1,random_state=42,class_weight = clwght)\n",
    "    \n",
    "    dt_fit.fit(x_train,y_train)\n",
    "    dt_wttune.loc[i, 'zero_wght'] = clwght[0]       \n",
    "    dt_wttune.loc[i, 'one_wght'] = clwght[1]     \n",
    "    dt_wttune.loc[i, 'tr_accuracy'] = round(accuracy_score(y_train,dt_fit.predict(x_train)),3)    \n",
    "    dt_wttune.loc[i, 'tst_accuracy'] = round(accuracy_score(y_test,dt_fit.predict(x_test)),3)    \n",
    "        \n",
    "    clf_sp = classification_report(y_test,dt_fit.predict(x_test)).split()\n",
    "    dt_wttune.loc[i, 'prec_zero'] = float(clf_sp[5])   \n",
    "    dt_wttune.loc[i, 'prec_one'] = float(clf_sp[10])   \n",
    "    dt_wttune.loc[i, 'prec_ovll'] = float(clf_sp[17])   \n",
    "    \n",
    "    dt_wttune.loc[i, 'recl_zero'] = float(clf_sp[6])   \n",
    "    dt_wttune.loc[i, 'recl_one'] = float(clf_sp[11])   \n",
    "    dt_wttune.loc[i, 'recl_ovll'] = float(clf_sp[18])\n",
    "    \n",
    "    print (\"\\nClass Weights\",clwght,\"Train accuracy:\",round(accuracy_score(y_train,dt_fit.predict(x_train)),3),\"Test accuracy:\",round(accuracy_score(y_test,dt_fit.predict(x_test)),3))\n",
    "    print (\"Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,dt_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding screenshot, we can seen that at class weight values of 0.3 (for zero) and 0.7 (for one) it is identifying a higher number of attriters (25 out of 61) without compromising test accuracy 83.9% using decision trees methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used in random forest are: n_estimators representing the number of individual decision trees used is 5000, maximum features selected are auto, which means it will select sqrt(p) for classification and p/3 for regression automatically. Here is the straightforward classification problem though. Minimum samples per leaf provides the minimum number of observations required in the terminal node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest - Train Confusion Matrix\n",
      "\n",
      " Predicted    0    1\n",
      "Actuall            \n",
      "0          841   12\n",
      "1           76  100\n",
      "\n",
      "Random Forest - Train accuracy 0.914\n",
      "\n",
      "Random Forest  - Train Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       853\n",
      "           1       0.89      0.57      0.69       176\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1029\n",
      "   macro avg       0.90      0.78      0.82      1029\n",
      "weighted avg       0.91      0.91      0.91      1029\n",
      "\n",
      "\n",
      "\n",
      "Random Forest - Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          373   7\n",
      "1           47  14\n",
      "\n",
      "Random Forest - Test accuracy 0.878\n",
      "\n",
      "Random Forest - Test Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       380\n",
      "           1       0.67      0.23      0.34        61\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       441\n",
      "   macro avg       0.78      0.61      0.64       441\n",
      "weighted avg       0.86      0.88      0.85       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_fit = RandomForestClassifier(n_estimators=5000, criterion=\"gini\", max_depth=5, min_samples_split=2,\n",
    "                                bootstrap=True, max_features='auto', random_state=42, min_samples_leaf=1, \n",
    "                                class_weight = {0:0.3,1:0.7})\n",
    "rf_fit.fit(x_train,y_train)       \n",
    "\n",
    "print (\"\\nRandom Forest - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,rf_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "print (\"\\nRandom Forest - Train accuracy\",round(accuracy_score(y_train,rf_fit.predict(x_train)),3))\n",
    "print (\"\\nRandom Forest  - Train Classification Report\\n\",classification_report(y_train,rf_fit.predict(x_train)))\n",
    "\n",
    "print (\"\\n\\nRandom Forest - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,rf_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "print (\"\\nRandom Forest - Test accuracy\",round(accuracy_score(y_test,rf_fit.predict(x_test)),3))\n",
    "print (\"\\nRandom Forest - Test Classification Report\\n\",classification_report(y_test,rf_fit.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier produced 87.8% test accuracy compared with bagging 87.3%, and also identifies 14 actually attrited employees in contrast with bagging, for which 13 attrited employees have been identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot of Variable importance by mean decrease in gini\n",
    "model_ranks = pd.Series(rf_fit.feature_importances_,index=x_train.columns, name='Importance').sort_values(ascending=False, inplace=False)\n",
    "model_ranks.index.name = 'Variables'\n",
    "top_features = model_ranks.iloc[:31].sort_values(ascending=True,inplace=False)\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = top_features.plot(kind='barh')\n",
    "_ = ax.set_title(\"Variable Importance Plot\")\n",
    "_ = ax.set_xlabel('Mean decrease in Variance')\n",
    "_ = ax.set_yticklabels(top_features.index, fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the variable importance plot, it seems that the monthly income variable seems to be most significant, followed by overtime, total working years, stock option levels, years at company, and so on. This provides us with some insight into what are major contributing factors that determine whether the employee will remain with the company or leave the organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning parameters in a machine learning model plays a critical role. Here, we are showing a grid search example on how to tune a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier - Grid Search \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV \n",
    " \n",
    "pipeline = Pipeline([ ('clf',RandomForestClassifier(criterion='gini',class_weight = {0:0.3,1:0.7}))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning parameters are similar to random forest parameters apart from verifying all the combinations using the pipeline function. The number of combinations to be evaluated will be (3 x 3 x 2 x 2) *5 =36*5 = 180 combinations. Here 5 is used in the end, due to the cross validation of five-fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training score: 0.867\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 5\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "\tclf__n_estimators: 3000\n",
      "Testing accuracy: 0.873\n",
      "\n",
      "Complete report of Testing data\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       380\n",
      "           1       0.62      0.21      0.32        61\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       441\n",
      "   macro avg       0.75      0.60      0.62       441\n",
      "weighted avg       0.85      0.87      0.85       441\n",
      "\n",
      "\n",
      "\n",
      "Random Forest Grid Search- Test Confusion Matrix\n",
      "\n",
      " Predicted    0   1\n",
      "Actuall           \n",
      "0          372   8\n",
      "1           48  13\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'clf__n_estimators':(2000,3000,5000),\n",
    "        'clf__max_depth':(5,15,30),\n",
    "        'clf__min_samples_split':(2,3),\n",
    "        'clf__min_samples_leaf':(1,2)  }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,parameters,n_jobs=-1,cv=5,verbose=1,scoring='accuracy')\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "print ('Best Training score: %0.3f' % grid_search.best_score_)\n",
    "print ('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "predictions = grid_search.predict(x_test)\n",
    "\n",
    "print (\"Testing accuracy:\",round(accuracy_score(y_test, predictions),4))\n",
    "print (\"\\nComplete report of Testing data\\n\",classification_report(y_test, predictions))\n",
    "print (\"\\n\\nRandom Forest Grid Search- Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, predictions,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding results, grid search seems to not provide much advantage compared with the already explored random forest result. But, practically, most of the times, it will provide better and more robust results compared with a simple exploration of models. However, by carefully evaluating many different combinations, it will eventually discover the best parameters combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "dtree = DecisionTreeClassifier(criterion='gini',max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision stump is used as a base classifier for AdaBoost. If we observe the following code, the depth of the tree remains as 1, which has decision taking ability only once (also considered a weak classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='gini',max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AdaBoost, decision stump has been used as a base estimator to fit on whole datasets and then fits additional copies of the classifier on the same dataset up to 5000 times. The learning rate shrinks the contribution of each classifer by 0.05. There is a trade-off between learning rate and the number of estimators. By carefully choosing a low learning rate and a long number of estimators, one can converge optimum very much, however at the expense of computing power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost - Train Confusion Matrix\n",
      "\n",
      " Predicted    0    1\n",
      "Actuall            \n",
      "0          844    9\n",
      "1           55  121\n",
      "\n",
      "AdaBoost - Train accuracy 0.938\n",
      "\n",
      "AdaBoost  - Train Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       853\n",
      "           1       0.93      0.69      0.79       176\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1029\n",
      "   macro avg       0.93      0.84      0.88      1029\n",
      "weighted avg       0.94      0.94      0.93      1029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adabst_fit = AdaBoostClassifier(base_estimator= dtree,n_estimators=5000,learning_rate=0.05,random_state=42)\n",
    "\n",
    "adabst_fit.fit(x_train, y_train)\n",
    "print (\"\\nAdaBoost - Train Confusion Matrix\\n\\n\", pd.crosstab(y_train, adabst_fit.predict(x_train), rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "print (\"\\nAdaBoost - Train accuracy\",round(accuracy_score(y_train,adabst_fit.predict(x_train)), 3))\n",
    "print (\"\\nAdaBoost  - Train Classification Report\\n\",classification_report(y_train,adabst_fit.predict(x_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the AdaBoost seems to be much better than the known best random forest classifiers in terms of the recall of 1 value. Though there is a slight decrease in accuracy to 86.8% compared with the best accuracy of 87.8%, the number of 1's predicted is 23 from the RF, which is 14 with some expense of increase in 0's, but it really made good progress in terms of identifying actual attriters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the competition-winning algorithms that work on the principle of boosting weak learners iteratively by shifting focus towards problematic observations that were difficult to predict in previous iterations and performing an ensemble of weak learners, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, but it generalizes them by allowing optimization of an arbitrary differentiable loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientboost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters used in the gradient boosting algorithms are as follows. Deviance has been used for loss, as the problem we are trying to solve is 0/1 binary classification. The learning rate has been chosen as 0.05, number of trees to build is 5000 trees, minimum sample per leaf/terminal node is 1, and minimum samples needed in a bucket for qualification for splitting is 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-94d43e0971a0>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-94d43e0971a0>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print (\"\\nGradient Boost - Test accuracy\",round(accuracy_score(y_test, gbc_fit.predict(x_test)),3)) >>> print (\"\\nGradient Boost - Test Classification Report\\n\",classification_report( y_test, gbc_fit.predict(x_test)))\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gbc_fit = GradientBoostingClassifier (loss='deviance', learning_rate=0.05, n_estimators=5000, min_samples_split=2, min_samples_leaf=1, max_depth=1, random_state=42 ) \n",
    "\n",
    "gbc_fit.fit(x_train,y_train) \n",
    "\n",
    "print (\"\\nGradient Boost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, gbc_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "print (\"\\nGradient Boost - Train accuracy\",round(accuracy_score(y_train, gbc_fit.predict(x_train)),3))\n",
    "print (\"\\nGradient Boost - Train Classification Report\\n\",classification_report( y_train, gbc_fit.predict(x_train)))\n",
    "\n",
    "print (\"\\n\\nGradient Boost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, gbc_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "print (\"\\nGradient Boost - Test accuracy\",round(accuracy_score(y_test, gbc_fit.predict(x_test)),3)) >>> print (\"\\nGradient Boost - Test Classification Report\\n\",classification_report( y_test, gbc_fit.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we analyze the results, Gradient boosting has given better results than AdaBoost with the highest possible test accuracy of 87.5% with most 1's captured as 24, compared with AdaBoost with which the test accuracy obtained was 86.8%. Hence, it has been proven that it is no wonder why every data scientist tries to use this algorithm to win competitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme gradient boosting - XGBoost classifier\n",
    "XGBoost is the new algorithm developed in 2014 by Tianqi Chen based on the Gradient boosting principles. It has created a storm in the data science community since its inception. XGBoost has been developed with both deep consideration in terms of system optimization and principles in machine learning. The goal of the library is to push the extremes of the computation limits of machines to provide scalable, portable, and accurate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost Classifier\n",
    "import xgboost as xgb\n",
    "xgb_fit = xgb.XGBClassifier(max_depth=2, n_estimators=5000, learning_rate=0.05)\n",
    "xgb_fit.fit(x_train, y_train)\n",
    "\n",
    "print (\"\\nXGBoost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, xgb_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))     \n",
    "print (\"\\nXGBoost - Train accuracy\",round(accuracy_score(y_train, xgb_fit.predict(x_train)),3))\n",
    "print (\"\\nXGBoost  - Train Classification Report\\n\",classification_report(y_train, xgb_fit.predict(x_train)))\n",
    "print (\"\\n\\nXGBoost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, xgb_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))     \n",
    "print (\"\\nXGBoost - Test accuracy\",round(accuracy_score(y_test, xgb_fit.predict(x_test)),3))\n",
    "print (\"\\nXGBoost - Test Classification Report\\n\",classification_report(y_test, xgb_fit.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained from XGBoost are almost similar to gradient boosting. The test accuracy obtained was 87.1%, whereas boosting got 87.5%, and also the number of 1's identified is 23 compared with 24 in gradient boosting. The greatest advantage of XGBoost over Gradient boost is in terms of performance and the options available to control model tune. By changing a few of them, makes XGBoost even beat gradient boost as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
